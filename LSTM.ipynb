{"cells":[{"cell_type":"markdown","id":"ac435407","metadata":{"id":"ac435407"},"source":["# Test NSEG dataset"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"656qX158tZ-Q","executionInfo":{"status":"ok","timestamp":1685451234617,"user_tz":-120,"elapsed":19405,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}},"outputId":"e5366c0f-dc39-4e82-ca91-686cbd22a6c1"},"id":"656qX158tZ-Q","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"id":"97dbc297","metadata":{"id":"97dbc297","executionInfo":{"status":"ok","timestamp":1685451367400,"user_tz":-120,"elapsed":4893,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}}},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import LSTM, GRU, Dense, Masking, Bidirectional\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import LeaveOneGroupOut\n","from sklearn.model_selection import cross_val_score\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"id":"fbb19c03","metadata":{"scrolled":false,"id":"fbb19c03","executionInfo":{"status":"ok","timestamp":1685451368193,"user_tz":-120,"elapsed":801,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}}},"outputs":[],"source":["df = pd.read_csv(\"/content/drive/MyDrive/FVAB/dataset_de_LDS_SEEDIV.csv\")\n","df.drop(['Unnamed: 0'],axis=1, inplace=True)\n","df = df.set_index(['id_user','session','video'])"]},{"cell_type":"code","execution_count":8,"id":"368c1600","metadata":{"id":"368c1600","executionInfo":{"status":"ok","timestamp":1685451452532,"user_tz":-120,"elapsed":277,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}}},"outputs":[],"source":["df[df.isnull()==True] = -10"]},{"cell_type":"code","execution_count":9,"id":"46a87d7a","metadata":{"id":"46a87d7a","executionInfo":{"status":"ok","timestamp":1685451454061,"user_tz":-120,"elapsed":2,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}}},"outputs":[],"source":["def preprocessing_for_LSTM(X_train, X_test, y_train, y_test):\n","    X_train = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))\n","    y_train = to_categorical(y_train)\n","    X_test = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))\n","    y_test = to_categorical(y_test)\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"markdown","id":"bee67aad","metadata":{"id":"bee67aad"},"source":["# Subject dependent"]},{"cell_type":"code","execution_count":10,"id":"15d96888","metadata":{"id":"15d96888","executionInfo":{"status":"ok","timestamp":1685451456032,"user_tz":-120,"elapsed":317,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}}},"outputs":[],"source":["def subject_dependent_split(df,session):\n","    df = df.reset_index()\n","    np.random.seed(75)\n","    test_videos = np.random.choice(np.arange(1, 25), replace=False, size=(8))\n","    df_sess = df.loc[df['session'] == session]\n","    X_test = df_sess[df_sess['video'].isin(test_videos)].set_index(['id_user','session','video']).drop('emotion',axis=1)\n","    y_test = df_sess[df_sess['video'].isin(test_videos)].set_index(['id_user','session','video']).emotion\n","    X_train = df_sess[~df_sess['video'].isin(test_videos)].set_index(['id_user','session','video']).drop('emotion',axis=1)\n","    y_train = df_sess[~df_sess['video'].isin(test_videos)].set_index(['id_user','session','video']).emotion\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"id":"2z5YjWh_vFLd","executionInfo":{"status":"ok","timestamp":1685451472254,"user_tz":-120,"elapsed":484,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}},"outputId":"32caa66d-64b7-46f5-ccef-71a30602bc66"},"id":"2z5YjWh_vFLd","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                               0          1          2          3          4  \\\n","id_user session video                                                          \n","10      1       1      25.622467  22.612988  21.225053  19.946429  19.266408   \n","                2      25.496412  22.407150  20.974761  20.547296  20.101432   \n","                3      25.302885  22.570660  21.633718  21.230311  21.011355   \n","                4      25.939116  22.808869  21.332588  19.427715  18.515616   \n","                5      25.503216  22.350826  21.157540  19.464702  18.714055   \n","...                          ...        ...        ...        ...        ...   \n","9       3       20     25.031688  22.242642  21.493354  20.546762  20.240579   \n","                21     25.676278  22.638414  21.002611  20.324261  20.132503   \n","                22     25.323145  22.412124  21.821257  20.533943  20.393856   \n","                23     24.879851  22.047250  21.202716  20.121354  19.557465   \n","                24     25.481525  22.404174  21.688045  20.494014  20.080786   \n","\n","                               5          6          7          8          9  \\\n","id_user session video                                                          \n","10      1       1      23.396372  20.964809  20.035117  17.815367  16.887869   \n","                2      23.023210  21.083666  19.698849  17.761071  16.874142   \n","                3      23.156935  21.018987  20.335182  18.201790  17.557549   \n","                4      22.954357  20.906463  20.060412  17.828035  16.748080   \n","                5      23.024172  20.880395  20.269262  17.854670  16.799909   \n","...                          ...        ...        ...        ...        ...   \n","9       3       20     21.793385  18.860982  18.035320  16.896775  16.196043   \n","                21     21.736976  18.712277  17.463331  16.739365  16.433142   \n","                22     21.565260  18.816661  18.326217  17.525892  17.510356   \n","                23     21.748668  18.707098  17.733498  16.605061  15.808999   \n","                24     21.794618  18.929078  18.257980  16.976043  16.482274   \n","\n","                       ...  1590  1591  1592  1593  1594  1595  1596  1597  \\\n","id_user session video  ...                                                   \n","10      1       1      ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","                2      ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","                3      ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","                4      ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","                5      ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","...                    ...   ...   ...   ...   ...   ...   ...   ...   ...   \n","9       3       20     ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","                21     ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","                22     ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","                23     ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","                24     ... -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0   \n","\n","                       1598  1599  \n","id_user session video              \n","10      1       1     -10.0 -10.0  \n","                2     -10.0 -10.0  \n","                3     -10.0 -10.0  \n","                4     -10.0 -10.0  \n","                5     -10.0 -10.0  \n","...                     ...   ...  \n","9       3       20    -10.0 -10.0  \n","                21    -10.0 -10.0  \n","                22    -10.0 -10.0  \n","                23    -10.0 -10.0  \n","                24    -10.0 -10.0  \n","\n","[1080 rows x 1601 columns]"],"text/html":["\n","  <div id=\"df-740e8782-116a-4894-81e5-173476fbf16a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>1590</th>\n","      <th>1591</th>\n","      <th>1592</th>\n","      <th>1593</th>\n","      <th>1594</th>\n","      <th>1595</th>\n","      <th>1596</th>\n","      <th>1597</th>\n","      <th>1598</th>\n","      <th>1599</th>\n","    </tr>\n","    <tr>\n","      <th>id_user</th>\n","      <th>session</th>\n","      <th>video</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">10</th>\n","      <th rowspan=\"5\" valign=\"top\">1</th>\n","      <th>1</th>\n","      <td>25.622467</td>\n","      <td>22.612988</td>\n","      <td>21.225053</td>\n","      <td>19.946429</td>\n","      <td>19.266408</td>\n","      <td>23.396372</td>\n","      <td>20.964809</td>\n","      <td>20.035117</td>\n","      <td>17.815367</td>\n","      <td>16.887869</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>25.496412</td>\n","      <td>22.407150</td>\n","      <td>20.974761</td>\n","      <td>20.547296</td>\n","      <td>20.101432</td>\n","      <td>23.023210</td>\n","      <td>21.083666</td>\n","      <td>19.698849</td>\n","      <td>17.761071</td>\n","      <td>16.874142</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>25.302885</td>\n","      <td>22.570660</td>\n","      <td>21.633718</td>\n","      <td>21.230311</td>\n","      <td>21.011355</td>\n","      <td>23.156935</td>\n","      <td>21.018987</td>\n","      <td>20.335182</td>\n","      <td>18.201790</td>\n","      <td>17.557549</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>25.939116</td>\n","      <td>22.808869</td>\n","      <td>21.332588</td>\n","      <td>19.427715</td>\n","      <td>18.515616</td>\n","      <td>22.954357</td>\n","      <td>20.906463</td>\n","      <td>20.060412</td>\n","      <td>17.828035</td>\n","      <td>16.748080</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>25.503216</td>\n","      <td>22.350826</td>\n","      <td>21.157540</td>\n","      <td>19.464702</td>\n","      <td>18.714055</td>\n","      <td>23.024172</td>\n","      <td>20.880395</td>\n","      <td>20.269262</td>\n","      <td>17.854670</td>\n","      <td>16.799909</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">9</th>\n","      <th rowspan=\"5\" valign=\"top\">3</th>\n","      <th>20</th>\n","      <td>25.031688</td>\n","      <td>22.242642</td>\n","      <td>21.493354</td>\n","      <td>20.546762</td>\n","      <td>20.240579</td>\n","      <td>21.793385</td>\n","      <td>18.860982</td>\n","      <td>18.035320</td>\n","      <td>16.896775</td>\n","      <td>16.196043</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>25.676278</td>\n","      <td>22.638414</td>\n","      <td>21.002611</td>\n","      <td>20.324261</td>\n","      <td>20.132503</td>\n","      <td>21.736976</td>\n","      <td>18.712277</td>\n","      <td>17.463331</td>\n","      <td>16.739365</td>\n","      <td>16.433142</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>25.323145</td>\n","      <td>22.412124</td>\n","      <td>21.821257</td>\n","      <td>20.533943</td>\n","      <td>20.393856</td>\n","      <td>21.565260</td>\n","      <td>18.816661</td>\n","      <td>18.326217</td>\n","      <td>17.525892</td>\n","      <td>17.510356</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>24.879851</td>\n","      <td>22.047250</td>\n","      <td>21.202716</td>\n","      <td>20.121354</td>\n","      <td>19.557465</td>\n","      <td>21.748668</td>\n","      <td>18.707098</td>\n","      <td>17.733498</td>\n","      <td>16.605061</td>\n","      <td>15.808999</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>25.481525</td>\n","      <td>22.404174</td>\n","      <td>21.688045</td>\n","      <td>20.494014</td>\n","      <td>20.080786</td>\n","      <td>21.794618</td>\n","      <td>18.929078</td>\n","      <td>18.257980</td>\n","      <td>16.976043</td>\n","      <td>16.482274</td>\n","      <td>...</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","      <td>-10.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1080 rows × 1601 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-740e8782-116a-4894-81e5-173476fbf16a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-740e8782-116a-4894-81e5-173476fbf16a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-740e8782-116a-4894-81e5-173476fbf16a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":24,"id":"1ff4048a","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1ff4048a","executionInfo":{"status":"error","timestamp":1685456823012,"user_tz":-120,"elapsed":1387996,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}},"outputId":"43b2842f-4dcc-4619-dd88-5314fb85ce76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/300\n","120/120 [==============================] - 14s 65ms/step - loss: 1.4340 - accuracy: 0.3042\n","Epoch 2/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.3607 - accuracy: 0.3208\n","Epoch 3/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.3781 - accuracy: 0.3042\n","Epoch 4/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.3610 - accuracy: 0.3250\n","Epoch 5/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.3674 - accuracy: 0.3167\n","Epoch 6/300\n","120/120 [==============================] - 8s 71ms/step - loss: 1.3568 - accuracy: 0.3333\n","Epoch 7/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.3552 - accuracy: 0.3208\n","Epoch 8/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.3466 - accuracy: 0.3208\n","Epoch 9/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.3543 - accuracy: 0.3000\n","Epoch 10/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.3519 - accuracy: 0.3250\n","Epoch 11/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.3454 - accuracy: 0.3167\n","Epoch 12/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.3467 - accuracy: 0.3083\n","Epoch 13/300\n","120/120 [==============================] - 7s 54ms/step - loss: 1.3480 - accuracy: 0.3000\n","Epoch 14/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.3457 - accuracy: 0.3083\n","Epoch 15/300\n","120/120 [==============================] - 7s 58ms/step - loss: 1.3361 - accuracy: 0.3167\n","Epoch 16/300\n","120/120 [==============================] - 9s 74ms/step - loss: 1.3347 - accuracy: 0.3542\n","Epoch 17/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.3348 - accuracy: 0.3208\n","Epoch 18/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.3248 - accuracy: 0.3583\n","Epoch 19/300\n","120/120 [==============================] - 7s 58ms/step - loss: 1.3312 - accuracy: 0.3458\n","Epoch 20/300\n","120/120 [==============================] - 7s 59ms/step - loss: 1.3271 - accuracy: 0.3333\n","Epoch 21/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.3272 - accuracy: 0.3375\n","Epoch 22/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.3289 - accuracy: 0.3833\n","Epoch 23/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.3285 - accuracy: 0.2833\n","Epoch 24/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.3257 - accuracy: 0.3500\n","Epoch 25/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.3230 - accuracy: 0.3500\n","Epoch 26/300\n","120/120 [==============================] - 6s 54ms/step - loss: 1.3215 - accuracy: 0.3833\n","Epoch 27/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.3214 - accuracy: 0.3125\n","Epoch 28/300\n","120/120 [==============================] - 6s 54ms/step - loss: 1.3173 - accuracy: 0.3500\n","Epoch 29/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.3116 - accuracy: 0.3875\n","Epoch 30/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.3168 - accuracy: 0.3500\n","Epoch 31/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.3136 - accuracy: 0.4042\n","Epoch 32/300\n","120/120 [==============================] - 7s 54ms/step - loss: 1.3077 - accuracy: 0.3792\n","Epoch 33/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.3115 - accuracy: 0.3333\n","Epoch 34/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.3061 - accuracy: 0.3708\n","Epoch 35/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.3013 - accuracy: 0.4000\n","Epoch 36/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.3114 - accuracy: 0.3583\n","Epoch 37/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.2992 - accuracy: 0.3917\n","Epoch 38/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.3070 - accuracy: 0.3542\n","Epoch 39/300\n","120/120 [==============================] - 7s 59ms/step - loss: 1.2920 - accuracy: 0.4042\n","Epoch 40/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.3083 - accuracy: 0.3625\n","Epoch 41/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.2940 - accuracy: 0.3583\n","Epoch 42/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.3005 - accuracy: 0.3875\n","Epoch 43/300\n","120/120 [==============================] - 7s 54ms/step - loss: 1.2907 - accuracy: 0.3542\n","Epoch 44/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.3016 - accuracy: 0.3625\n","Epoch 45/300\n","120/120 [==============================] - 6s 54ms/step - loss: 1.2855 - accuracy: 0.4167\n","Epoch 46/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.2938 - accuracy: 0.3708\n","Epoch 47/300\n","120/120 [==============================] - 6s 53ms/step - loss: 1.2775 - accuracy: 0.4083\n","Epoch 48/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.2826 - accuracy: 0.3500\n","Epoch 49/300\n","120/120 [==============================] - 6s 53ms/step - loss: 1.2867 - accuracy: 0.4333\n","Epoch 50/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.2801 - accuracy: 0.3750\n","Epoch 51/300\n","120/120 [==============================] - 7s 54ms/step - loss: 1.2795 - accuracy: 0.3917\n","Epoch 52/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.2778 - accuracy: 0.3958\n","Epoch 53/300\n","120/120 [==============================] - 6s 54ms/step - loss: 1.2704 - accuracy: 0.4167\n","Epoch 54/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.2816 - accuracy: 0.3708\n","Epoch 55/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.2625 - accuracy: 0.4333\n","Epoch 56/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.2673 - accuracy: 0.3708\n","Epoch 57/300\n","120/120 [==============================] - 7s 58ms/step - loss: 1.2604 - accuracy: 0.4125\n","Epoch 58/300\n","120/120 [==============================] - 8s 70ms/step - loss: 1.2561 - accuracy: 0.4083\n","Epoch 59/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.2531 - accuracy: 0.3958\n","Epoch 60/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.2489 - accuracy: 0.3917\n","Epoch 61/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.2493 - accuracy: 0.4042\n","Epoch 62/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.2720 - accuracy: 0.3500\n","Epoch 63/300\n","120/120 [==============================] - 8s 67ms/step - loss: 1.2435 - accuracy: 0.4167\n","Epoch 64/300\n","120/120 [==============================] - 7s 58ms/step - loss: 1.2427 - accuracy: 0.4417\n","Epoch 65/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.2299 - accuracy: 0.4167\n","Epoch 66/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.2327 - accuracy: 0.4167\n","Epoch 67/300\n","120/120 [==============================] - 8s 68ms/step - loss: 1.2306 - accuracy: 0.4125\n","Epoch 68/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.2449 - accuracy: 0.4208\n","Epoch 69/300\n","120/120 [==============================] - 7s 63ms/step - loss: 1.2234 - accuracy: 0.4333\n","Epoch 70/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.2345 - accuracy: 0.4375\n","Epoch 71/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.2034 - accuracy: 0.4667\n","Epoch 72/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.2369 - accuracy: 0.4208\n","Epoch 73/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.2443 - accuracy: 0.3875\n","Epoch 74/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.2203 - accuracy: 0.4667\n","Epoch 75/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.2213 - accuracy: 0.4250\n","Epoch 76/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.2174 - accuracy: 0.3833\n","Epoch 77/300\n","120/120 [==============================] - 7s 54ms/step - loss: 1.2162 - accuracy: 0.4292\n","Epoch 78/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.2236 - accuracy: 0.4208\n","Epoch 79/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.1972 - accuracy: 0.4500\n","Epoch 80/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.2188 - accuracy: 0.4583\n","Epoch 81/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.2484 - accuracy: 0.4292\n","Epoch 82/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.2060 - accuracy: 0.4292\n","Epoch 83/300\n","120/120 [==============================] - 7s 59ms/step - loss: 1.2180 - accuracy: 0.4042\n","Epoch 84/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.2322 - accuracy: 0.4083\n","Epoch 85/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.2039 - accuracy: 0.3958\n","Epoch 86/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.2034 - accuracy: 0.4417\n","Epoch 87/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.2105 - accuracy: 0.4125\n","Epoch 88/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.1997 - accuracy: 0.4167\n","Epoch 89/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.1666 - accuracy: 0.4625\n","Epoch 90/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.1554 - accuracy: 0.4833\n","Epoch 91/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.1803 - accuracy: 0.4458\n","Epoch 92/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.1574 - accuracy: 0.4875\n","Epoch 93/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.1603 - accuracy: 0.4500\n","Epoch 94/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.1743 - accuracy: 0.4500\n","Epoch 95/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.1879 - accuracy: 0.4375\n","Epoch 96/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.1574 - accuracy: 0.4542\n","Epoch 97/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.1509 - accuracy: 0.4917\n","Epoch 98/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.2038 - accuracy: 0.4333\n","Epoch 99/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.1344 - accuracy: 0.4625\n","Epoch 100/300\n","120/120 [==============================] - 9s 76ms/step - loss: 1.1720 - accuracy: 0.4708\n","Epoch 101/300\n","120/120 [==============================] - 7s 59ms/step - loss: 1.1591 - accuracy: 0.4458\n","Epoch 102/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.1233 - accuracy: 0.5125\n","Epoch 103/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.1284 - accuracy: 0.4958\n","Epoch 104/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.1292 - accuracy: 0.4583\n","Epoch 105/300\n","120/120 [==============================] - 7s 61ms/step - loss: 1.1186 - accuracy: 0.5000\n","Epoch 106/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.1168 - accuracy: 0.4958\n","Epoch 107/300\n","120/120 [==============================] - 8s 67ms/step - loss: 1.0770 - accuracy: 0.5000\n","Epoch 108/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.1424 - accuracy: 0.4667\n","Epoch 109/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.1766 - accuracy: 0.4417\n","Epoch 110/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.1658 - accuracy: 0.4625\n","Epoch 111/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.1307 - accuracy: 0.4792\n","Epoch 112/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.0981 - accuracy: 0.4750\n","Epoch 113/300\n","120/120 [==============================] - 8s 68ms/step - loss: 1.0973 - accuracy: 0.5125\n","Epoch 114/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.0745 - accuracy: 0.5458\n","Epoch 115/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.1502 - accuracy: 0.4583\n","Epoch 116/300\n","120/120 [==============================] - 8s 68ms/step - loss: 1.1217 - accuracy: 0.4708\n","Epoch 117/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.2215 - accuracy: 0.4333\n","Epoch 118/300\n","120/120 [==============================] - 8s 70ms/step - loss: 1.0821 - accuracy: 0.5000\n","Epoch 119/300\n","120/120 [==============================] - 7s 59ms/step - loss: 1.1141 - accuracy: 0.4958\n","Epoch 120/300\n","120/120 [==============================] - 8s 67ms/step - loss: 1.0806 - accuracy: 0.4917\n","Epoch 121/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.3475 - accuracy: 0.3625\n","Epoch 122/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.1171 - accuracy: 0.5000\n","Epoch 123/300\n","120/120 [==============================] - 8s 67ms/step - loss: 1.0672 - accuracy: 0.5250\n","Epoch 124/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.0564 - accuracy: 0.5250\n","Epoch 125/300\n","120/120 [==============================] - 8s 67ms/step - loss: 1.0433 - accuracy: 0.5167\n","Epoch 126/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.0831 - accuracy: 0.5333\n","Epoch 127/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.3700 - accuracy: 0.3833\n","Epoch 128/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.1669 - accuracy: 0.4333\n","Epoch 129/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.0255 - accuracy: 0.5333\n","Epoch 130/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.2064 - accuracy: 0.4458\n","Epoch 131/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.1206 - accuracy: 0.5042\n","Epoch 132/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.0341 - accuracy: 0.5333\n","Epoch 133/300\n","120/120 [==============================] - 7s 61ms/step - loss: 1.0887 - accuracy: 0.5042\n","Epoch 134/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.1214 - accuracy: 0.4833\n","Epoch 135/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.0312 - accuracy: 0.5292\n","Epoch 136/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.0948 - accuracy: 0.4875\n","Epoch 137/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.1974 - accuracy: 0.4125\n","Epoch 138/300\n","120/120 [==============================] - 10s 84ms/step - loss: 1.0822 - accuracy: 0.4917\n","Epoch 139/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.0369 - accuracy: 0.5167\n","Epoch 140/300\n","120/120 [==============================] - 9s 71ms/step - loss: 1.0222 - accuracy: 0.5458\n","Epoch 141/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.0390 - accuracy: 0.5250\n","Epoch 142/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.1110 - accuracy: 0.4958\n","Epoch 143/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.0148 - accuracy: 0.5500\n","Epoch 144/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.0431 - accuracy: 0.5083\n","Epoch 145/300\n","120/120 [==============================] - 8s 67ms/step - loss: 1.0785 - accuracy: 0.5167\n","Epoch 146/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.1700 - accuracy: 0.4917\n","Epoch 147/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.2329 - accuracy: 0.4333\n","Epoch 148/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.2164 - accuracy: 0.4458\n","Epoch 149/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.0220 - accuracy: 0.5375\n","Epoch 150/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.0655 - accuracy: 0.5167\n","Epoch 151/300\n","120/120 [==============================] - 7s 58ms/step - loss: 1.0911 - accuracy: 0.5042\n","Epoch 152/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.0320 - accuracy: 0.5333\n","Epoch 153/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.0230 - accuracy: 0.5250\n","Epoch 154/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.2616 - accuracy: 0.4375\n","Epoch 155/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.0991 - accuracy: 0.4875\n","Epoch 156/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.0069 - accuracy: 0.5417\n","Epoch 157/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.0767 - accuracy: 0.5208\n","Epoch 158/300\n","120/120 [==============================] - 8s 65ms/step - loss: 0.9864 - accuracy: 0.5417\n","Epoch 159/300\n","120/120 [==============================] - 7s 58ms/step - loss: 1.1218 - accuracy: 0.4750\n","Epoch 160/300\n","120/120 [==============================] - 7s 62ms/step - loss: 1.3585 - accuracy: 0.4042\n","Epoch 161/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.3254 - accuracy: 0.3000\n","Epoch 162/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.3248 - accuracy: 0.3250\n","Epoch 163/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.3078 - accuracy: 0.3250\n","Epoch 164/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.2953 - accuracy: 0.3458\n","Epoch 165/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.2758 - accuracy: 0.3375\n","Epoch 166/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.2649 - accuracy: 0.3708\n","Epoch 167/300\n","120/120 [==============================] - 8s 64ms/step - loss: 1.2802 - accuracy: 0.3458\n","Epoch 168/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.2594 - accuracy: 0.3250\n","Epoch 169/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.2757 - accuracy: 0.2875\n","Epoch 170/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.2462 - accuracy: 0.3750\n","Epoch 171/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.2570 - accuracy: 0.3417\n","Epoch 172/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.2620 - accuracy: 0.3750\n","Epoch 173/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.1309 - accuracy: 0.4583\n","Epoch 174/300\n","120/120 [==============================] - 8s 63ms/step - loss: 1.1208 - accuracy: 0.4833\n","Epoch 175/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.0528 - accuracy: 0.5208\n","Epoch 176/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.0548 - accuracy: 0.5167\n","Epoch 177/300\n","120/120 [==============================] - 7s 55ms/step - loss: 1.1393 - accuracy: 0.4750\n","Epoch 178/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.2531 - accuracy: 0.4083\n","Epoch 179/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.0725 - accuracy: 0.5042\n","Epoch 180/300\n","120/120 [==============================] - 8s 66ms/step - loss: 1.0445 - accuracy: 0.5292\n","Epoch 181/300\n","120/120 [==============================] - 7s 56ms/step - loss: 1.0697 - accuracy: 0.5125\n","Epoch 182/300\n","120/120 [==============================] - 9s 74ms/step - loss: 1.1357 - accuracy: 0.4417\n","Epoch 183/300\n","120/120 [==============================] - 7s 58ms/step - loss: 1.0970 - accuracy: 0.4625\n","Epoch 184/300\n","120/120 [==============================] - 7s 61ms/step - loss: 1.0151 - accuracy: 0.5375\n","Epoch 185/300\n","120/120 [==============================] - 7s 60ms/step - loss: 1.0585 - accuracy: 0.4958\n","Epoch 186/300\n","120/120 [==============================] - 7s 59ms/step - loss: 1.0461 - accuracy: 0.5083\n","Epoch 187/300\n","120/120 [==============================] - 8s 65ms/step - loss: 1.0196 - accuracy: 0.5333\n","Epoch 188/300\n","120/120 [==============================] - 7s 57ms/step - loss: 1.0498 - accuracy: 0.5042\n","Epoch 189/300\n"," 13/120 [==>...........................] - ETA: 6s - loss: 0.9973 - accuracy: 0.5385"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-36062a5ee269>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainSD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trainSD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_testSD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_testSD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0maccuracy_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["accuracy_test = []\n","for i in range(1,4):\n","    X_trainSD, X_testSD, y_trainSD, y_testSD = subject_dependent_split(df,i)\n","    X_trainSD, X_testSD, y_trainSD, y_testSD = preprocessing_for_LSTM(X_trainSD, X_testSD, y_trainSD, y_testSD)\n","    del model\n","    model = Sequential()\n","    model.add(Masking(mask_value=-10,input_shape=(X_trainSD.shape[1], 1)))\n","    model.add(Bidirectional(GRU(units=32, input_shape=(X_trainSD.shape[1], 1))))\n","    model.add(Dense(units=4, activation='sigmoid'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model.fit(X_trainSD, y_trainSD, epochs=300, batch_size=4)\n","    loss, accuracy = model.evaluate(X_testSD, y_testSD)\n","    accuracy_test.append(accuracy)"]},{"cell_type":"code","execution_count":null,"id":"2797ff58","metadata":{"id":"2797ff58","executionInfo":{"status":"aborted","timestamp":1685455213059,"user_tz":-120,"elapsed":18,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}}},"outputs":[],"source":["print(\"ACCURACY: \", np.array(accuracy_test).mean())"]},{"cell_type":"markdown","id":"1bba961c","metadata":{"id":"1bba961c"},"source":["# Subject-Biased Experiment\n"]},{"cell_type":"code","execution_count":31,"id":"d08dc39d","metadata":{"id":"d08dc39d","executionInfo":{"status":"ok","timestamp":1685457987363,"user_tz":-120,"elapsed":4,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}}},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(df.drop('emotion',axis=1), df.emotion, test_size=0.20, random_state=104)\n","X_train, X_test, y_train, y_test = preprocessing_for_LSTM(X_train, X_test, y_train, y_test)"]},{"cell_type":"code","execution_count":33,"id":"055b6d32","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"055b6d32","executionInfo":{"status":"ok","timestamp":1685459006643,"user_tz":-120,"elapsed":481028,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}},"outputId":"b2b22deb-a39e-4288-f49c-50c44ddb8aa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","54/54 [==============================] - 12s 95ms/step - loss: 1.4025 - accuracy: 0.2975\n","Epoch 2/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3877 - accuracy: 0.3218\n","Epoch 3/100\n","54/54 [==============================] - 5s 97ms/step - loss: 1.3795 - accuracy: 0.2894\n","Epoch 4/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3855 - accuracy: 0.3021\n","Epoch 5/100\n","54/54 [==============================] - 5s 83ms/step - loss: 1.3796 - accuracy: 0.3356\n","Epoch 6/100\n","54/54 [==============================] - 5s 96ms/step - loss: 1.3633 - accuracy: 0.3345\n","Epoch 7/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3634 - accuracy: 0.3368\n","Epoch 8/100\n","54/54 [==============================] - 5s 99ms/step - loss: 1.3689 - accuracy: 0.3194\n","Epoch 9/100\n","54/54 [==============================] - 5s 89ms/step - loss: 1.3711 - accuracy: 0.3287\n","Epoch 10/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3814 - accuracy: 0.3229\n","Epoch 11/100\n","54/54 [==============================] - 5s 89ms/step - loss: 1.3664 - accuracy: 0.3241\n","Epoch 12/100\n","54/54 [==============================] - 5s 87ms/step - loss: 1.3688 - accuracy: 0.3194\n","Epoch 13/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3614 - accuracy: 0.3611\n","Epoch 14/100\n","54/54 [==============================] - 5s 92ms/step - loss: 1.3603 - accuracy: 0.3310\n","Epoch 15/100\n","54/54 [==============================] - 5s 86ms/step - loss: 1.3665 - accuracy: 0.3206\n","Epoch 16/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3680 - accuracy: 0.2998\n","Epoch 17/100\n","54/54 [==============================] - 5s 95ms/step - loss: 1.3609 - accuracy: 0.3275\n","Epoch 18/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3627 - accuracy: 0.3287\n","Epoch 19/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3610 - accuracy: 0.3113\n","Epoch 20/100\n","54/54 [==============================] - 5s 96ms/step - loss: 1.3651 - accuracy: 0.3333\n","Epoch 21/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3632 - accuracy: 0.3391\n","Epoch 22/100\n","54/54 [==============================] - 5s 97ms/step - loss: 1.3564 - accuracy: 0.3322\n","Epoch 23/100\n","54/54 [==============================] - 5s 92ms/step - loss: 1.3614 - accuracy: 0.3275\n","Epoch 24/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3595 - accuracy: 0.3345\n","Epoch 25/100\n","54/54 [==============================] - 5s 86ms/step - loss: 1.3930 - accuracy: 0.2975\n","Epoch 26/100\n","54/54 [==============================] - 5s 90ms/step - loss: 1.3533 - accuracy: 0.3345\n","Epoch 27/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3523 - accuracy: 0.3252\n","Epoch 28/100\n","54/54 [==============================] - 5s 89ms/step - loss: 1.3515 - accuracy: 0.3229\n","Epoch 29/100\n","54/54 [==============================] - 5s 88ms/step - loss: 1.3609 - accuracy: 0.3194\n","Epoch 30/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3514 - accuracy: 0.3600\n","Epoch 31/100\n","54/54 [==============================] - 5s 90ms/step - loss: 1.3383 - accuracy: 0.3426\n","Epoch 32/100\n","54/54 [==============================] - 5s 86ms/step - loss: 1.3390 - accuracy: 0.3542\n","Epoch 33/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3445 - accuracy: 0.3356\n","Epoch 34/100\n","54/54 [==============================] - 5s 95ms/step - loss: 1.3426 - accuracy: 0.3380\n","Epoch 35/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3513 - accuracy: 0.3391\n","Epoch 36/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3493 - accuracy: 0.3426\n","Epoch 37/100\n","54/54 [==============================] - 5s 94ms/step - loss: 1.3503 - accuracy: 0.3403\n","Epoch 38/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3360 - accuracy: 0.3634\n","Epoch 39/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3589 - accuracy: 0.3345\n","Epoch 40/100\n","54/54 [==============================] - 5s 96ms/step - loss: 1.3367 - accuracy: 0.3484\n","Epoch 41/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3272 - accuracy: 0.3669\n","Epoch 42/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3370 - accuracy: 0.3507\n","Epoch 43/100\n","54/54 [==============================] - 5s 94ms/step - loss: 1.3251 - accuracy: 0.3600\n","Epoch 44/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3389 - accuracy: 0.3391\n","Epoch 45/100\n","54/54 [==============================] - 5s 86ms/step - loss: 1.3308 - accuracy: 0.3495\n","Epoch 46/100\n","54/54 [==============================] - 5s 91ms/step - loss: 1.3290 - accuracy: 0.3484\n","Epoch 47/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3176 - accuracy: 0.3692\n","Epoch 48/100\n","54/54 [==============================] - 5s 86ms/step - loss: 1.3388 - accuracy: 0.3611\n","Epoch 49/100\n","54/54 [==============================] - 5s 88ms/step - loss: 1.3373 - accuracy: 0.3403\n","Epoch 50/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3130 - accuracy: 0.3681\n","Epoch 51/100\n","54/54 [==============================] - 5s 88ms/step - loss: 1.3069 - accuracy: 0.3692\n","Epoch 52/100\n","54/54 [==============================] - 5s 88ms/step - loss: 1.3188 - accuracy: 0.3669\n","Epoch 53/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3051 - accuracy: 0.3623\n","Epoch 54/100\n","54/54 [==============================] - 5s 92ms/step - loss: 1.3153 - accuracy: 0.3542\n","Epoch 55/100\n","54/54 [==============================] - 5s 85ms/step - loss: 1.3121 - accuracy: 0.3866\n","Epoch 56/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3220 - accuracy: 0.3611\n","Epoch 57/100\n","54/54 [==============================] - 5s 95ms/step - loss: 1.3216 - accuracy: 0.3681\n","Epoch 58/100\n","54/54 [==============================] - 5s 83ms/step - loss: 1.3128 - accuracy: 0.3866\n","Epoch 59/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3131 - accuracy: 0.3634\n","Epoch 60/100\n","54/54 [==============================] - 5s 94ms/step - loss: 1.2893 - accuracy: 0.3854\n","Epoch 61/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3016 - accuracy: 0.3750\n","Epoch 62/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3498 - accuracy: 0.3426\n","Epoch 63/100\n","54/54 [==============================] - 5s 93ms/step - loss: 1.3341 - accuracy: 0.3542\n","Epoch 64/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3488 - accuracy: 0.3426\n","Epoch 65/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.3170 - accuracy: 0.3704\n","Epoch 66/100\n","54/54 [==============================] - 5s 95ms/step - loss: 1.3391 - accuracy: 0.3715\n","Epoch 67/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.3111 - accuracy: 0.3981\n","Epoch 68/100\n","54/54 [==============================] - 5s 85ms/step - loss: 1.2787 - accuracy: 0.4005\n","Epoch 69/100\n","54/54 [==============================] - 5s 92ms/step - loss: 1.2998 - accuracy: 0.3785\n","Epoch 70/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2868 - accuracy: 0.3854\n","Epoch 71/100\n","54/54 [==============================] - 5s 86ms/step - loss: 1.2764 - accuracy: 0.4005\n","Epoch 72/100\n","54/54 [==============================] - 5s 89ms/step - loss: 1.2986 - accuracy: 0.3681\n","Epoch 73/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2850 - accuracy: 0.3924\n","Epoch 74/100\n","54/54 [==============================] - 5s 90ms/step - loss: 1.2808 - accuracy: 0.4005\n","Epoch 75/100\n","54/54 [==============================] - 5s 88ms/step - loss: 1.2974 - accuracy: 0.3646\n","Epoch 76/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2849 - accuracy: 0.3935\n","Epoch 77/100\n","54/54 [==============================] - 5s 93ms/step - loss: 1.2739 - accuracy: 0.4005\n","Epoch 78/100\n","54/54 [==============================] - 5s 85ms/step - loss: 1.2787 - accuracy: 0.3981\n","Epoch 79/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2892 - accuracy: 0.3611\n","Epoch 80/100\n","54/54 [==============================] - 5s 94ms/step - loss: 1.2768 - accuracy: 0.4074\n","Epoch 81/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2802 - accuracy: 0.3738\n","Epoch 82/100\n","54/54 [==============================] - 4s 82ms/step - loss: 1.2672 - accuracy: 0.4074\n","Epoch 83/100\n","54/54 [==============================] - 5s 95ms/step - loss: 1.2622 - accuracy: 0.4120\n","Epoch 84/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2867 - accuracy: 0.3877\n","Epoch 85/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2767 - accuracy: 0.3958\n","Epoch 86/100\n","54/54 [==============================] - 5s 95ms/step - loss: 1.2790 - accuracy: 0.3785\n","Epoch 87/100\n","54/54 [==============================] - 5s 84ms/step - loss: 1.2603 - accuracy: 0.4028\n","Epoch 88/100\n","54/54 [==============================] - 5s 92ms/step - loss: 1.2479 - accuracy: 0.4039\n","Epoch 89/100\n","54/54 [==============================] - 6s 104ms/step - loss: 1.2564 - accuracy: 0.4213\n","Epoch 90/100\n","54/54 [==============================] - 5s 83ms/step - loss: 1.2515 - accuracy: 0.4097\n","Epoch 91/100\n","54/54 [==============================] - 5s 85ms/step - loss: 1.2576 - accuracy: 0.4028\n","Epoch 92/100\n","54/54 [==============================] - 5s 93ms/step - loss: 1.2407 - accuracy: 0.4086\n","Epoch 93/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2405 - accuracy: 0.4271\n","Epoch 94/100\n","54/54 [==============================] - 5s 88ms/step - loss: 1.2438 - accuracy: 0.3935\n","Epoch 95/100\n","54/54 [==============================] - 5s 88ms/step - loss: 1.2241 - accuracy: 0.4363\n","Epoch 96/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2528 - accuracy: 0.4086\n","Epoch 97/100\n","54/54 [==============================] - 5s 90ms/step - loss: 1.2551 - accuracy: 0.3958\n","Epoch 98/100\n","54/54 [==============================] - 5s 86ms/step - loss: 1.2444 - accuracy: 0.4051\n","Epoch 99/100\n","54/54 [==============================] - 4s 83ms/step - loss: 1.2803 - accuracy: 0.4051\n","Epoch 100/100\n","54/54 [==============================] - 5s 93ms/step - loss: 1.2897 - accuracy: 0.4016\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7febfa4c6a10>"]},"metadata":{},"execution_count":33}],"source":["del model\n","model = Sequential()\n","model.add(Masking(mask_value=-10,input_shape=(X_trainSD.shape[1], 1)))\n","model.add(Bidirectional(GRU(units=128, input_shape=(X_trainSD.shape[1], 1))))\n","model.add(Dense(units=4, activation='sigmoid'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=300, batch_size=16)"]},{"cell_type":"code","execution_count":34,"id":"3bd5d87a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3bd5d87a","executionInfo":{"status":"ok","timestamp":1685459016069,"user_tz":-120,"elapsed":3422,"user":{"displayName":"NICOLA PIO SANTORSA","userId":"18186694644348138501"}},"outputId":"807831b1-44c5-43d3-df11-277e513d469c"},"outputs":[{"output_type":"stream","name":"stdout","text":["7/7 [==============================] - 3s 52ms/step - loss: 1.1925 - accuracy: 0.4583\n"]}],"source":["loss, accuracy = model.evaluate(X_test, y_test)"]},{"cell_type":"markdown","id":"f214d68a","metadata":{"id":"f214d68a"},"source":["# Subject Indipendent"]},{"cell_type":"code","execution_count":null,"id":"1f541c9e","metadata":{"id":"1f541c9e"},"outputs":[],"source":["def subject_independent_split(df,session):\n","    df = df.reset_index()\n","    df_sess = df.loc[df['session'] == session]\n","    groups = df_sess['id_user']\n","    X = df_sess.set_index(['id_user','session','video']).drop('emotion',axis=1)\n","    y = df_sess.set_index(['id_user','session','video']).emotion\n","    return X, y, groups"]},{"cell_type":"markdown","id":"7c9beb5d","metadata":{"id":"7c9beb5d"},"source":["# TO DO: da implementare kfold\n","https://stackoverflow.com/questions/48085182/cross-validation-in-keras\n","\n","\n","\"\"\"\n","from sklearn.model_selection import RepeatedKFold, cross_val_score\n","from tensorflow.keras.models import * \n","from tensorflow.keras.layers import * \n","from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n","\n","def buildmodel():\n","    model= Sequential([\n","        Dense(10, activation=\"relu\"),\n","        Dense(5, activation=\"relu\"),\n","        Dense(1)\n","    ])\n","    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n","    return(model)\n","\n","estimator= KerasRegressor(build_fn=buildmodel, epochs=100, batch_size=10, verbose=0)\n","kfold= RepeatedKFold(n_splits=5, n_repeats=100)\n","results= cross_val_score(estimator, x, y, cv=kfold, n_jobs=2)  # 2 cpus\n","results.mean()  # Mean MSE\n","\"\"\""]},{"cell_type":"code","execution_count":null,"id":"759e4939","metadata":{"id":"759e4939"},"outputs":[],"source":["scores_test = []\n","logo = LeaveOneGroupOut()\n","for i in range(1,4):\n","    X_SI,y_SI,groups = subject_independent_split(df,i)\n","    X_SI = np.array(X_SI).reshape((X_SI.shape[0], X_SI.shape[1], 1))\n","    y_SI = to_categorical(y_SI)\n","    del model\n","    model = Sequential()\n","    model.add(LSTM(units=64, input_shape=(int(X_SI.shape[1])-15, 1)))\n","    model.add(Dense(units=4, activation='relu'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    scores_gbc = cross_val_score(model, X_SI, y_SI, cv=logo, verbose=1 , groups = groups, n_jobs = -1)\n","    scores_test.append(rfSD.scores.mean())"]},{"cell_type":"code","execution_count":null,"id":"93559f2e","metadata":{"scrolled":true,"id":"93559f2e"},"outputs":[],"source":["print(\"TEST:\", np.array(scores_test).mean())"]},{"cell_type":"code","execution_count":null,"id":"1e0887f5","metadata":{"id":"1e0887f5"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}